### **1. 프로젝트 제목**  
**Titanic 생존자 예측 모델 성능 비교**  

---

### **2. 프로젝트 설명 (간단한 개요)**  
Titanic 데이터셋에서 결측치를 다양한 방법으로 처리한 후, 여러 머신러닝 모델을 학습하여 성능을 비교하였다.  
특히, AutoML을 활용하여 최적의 조합을 찾고, 가장 높은 예측 성능을 보이는 모델을 선정하였다.  

---

### **3. 주요 기능 / 특징**  
✅ Titanic 데이터셋의 결측치 처리 방식 비교  
✅ 30개 이상의 데이터셋 조합 생성 및 모델 성능 분석  
✅ **H2O AutoML**을 활용한 최적 모델 탐색  
✅ 최적 모델 및 하이퍼파라미터 분석  
✅ 향후 개선 방향 제시  

---

### **4. 기술 스택**  
🛠 **프로그래밍 언어:** Python  
📊 **라이브러리:** Pandas, NumPy, Scikit-learn, H2O AutoML  
📈 **모델:** Gradient Boosting (GBM), XGBoost, Stacked Ensemble  

---

### **5. 프로젝트 상태**  
🚀 **완료** (성능 분석 및 최적 모델 선정 완료)  

---

### **6. 성과 및 결과**  
🏆 **최적 데이터셋 조합 및 성능**  

| 최적 데이터셋 | RMSE   | MSE    |  
|--------------|--------|--------|  
| A5_C2_E1    | 0.3527 | 0.1244 |  
| A5_C2_E2    | 0.3527 | 0.1244 |  
| A5_C3_E1    | 0.3548 | 0.1259 |  
| A5_C3_E2    | 0.3548 | 0.1259 |  

🎯 **최적 모델 성능 비교**  

| Model                        | RMSE   | MSE    |  
|------------------------------|--------|--------|  
| StackedEnsemble_BestOfFamily | 0.3527 | 0.1244 |  
| StackedEnsemble_AllModels    | 0.3558 | 0.1265 |  
| GBM_2                        | 0.3571 | 0.1275 |  
| GBM_3                        | 0.3606 | 0.1301 |  

📌 **결론:**  
- **Stacked Ensemble 모델이 가장 우수한 성능**을 기록  
- **Gradient Boosting (GBM) 계열 모델도 높은 성능**을 보였음  
- **XGBoost 및 GLM 모델은 상대적으로 성능이 낮았음**  

---

### **7. 프로젝트 이미지 / 영상**  
📷 **모델 비교 그래프 및 결과 분석 차트 (추가 가능)**  

---

### **8. 주요 목표 / 문제 해결**  
📌 **목표:** Titanic 데이터셋을 활용하여 생존자 예측 모델의 최적 조합을 찾는 것.  
📌 **문제 해결 방법:**  
1. **결측치 처리 방법을 비교하여 최적 조합을 찾음**  
2. **AutoML을 활용하여 다수의 모델을 자동 탐색**  
3. **최적의 모델 및 데이터 전처리 방식 선정**  

---

### **9. 팀 구성 (옵션)**  
👨‍💻 **개인 프로젝트**  

---

### **10. 프로젝트의 미래 방향성 (옵션)**  
📈 **향후 개선 방향:**  
✅ **Feature Engineering**을 추가하여 모델 성능 개선  
✅ **GBM/XGBoost 모델의 하이퍼파라미터 최적화**  
✅ **데이터 증강 기법 활용**하여 추가 성능 향상 가능  
